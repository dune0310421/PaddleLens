[
    {
        "repo": "PaddlePaddle/PaDiff",
        "number": 80,
        "title": "ä»£ç é˜…è¯»åŠæ–‡æ¡£æ›´æ–°",
        "body": "### ä¸€ã€é—®é¢˜æè¿° ğŸ“š\r\nåœ¨ PaDiff ä»“åº“è¿›è¡ŒåŠŸèƒ½æ›´æ–°åï¼Œä»“åº“çš„ä½¿ç”¨æ–‡æ¡£æœªè¿›è¡ŒåŒæ­¥æ›´æ–°ï¼Œä¸”ä»“åº“è¿˜ç¼ºä¹ä¸€ä¸ªæ–°ç‰ˆæœ¬çš„æ¨¡å—è®¾è®¡å›¾\r\n\r\n### äºŒã€ ä»»åŠ¡ç›®æ ‡ ğŸš€\r\nå¯¹docsç›®å½•ä¸‹çš„æ–‡æ¡£è¿›è¡Œæ›´æ–°ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†æ–‡æ¡£æ˜¯æ—§ç‰ˆæœ¬çš„ï¼Œå°†å®ƒä»¬é€‚é…åˆ°æ–°ç‰ˆæœ¬\r\n\r\n### ä¸‰ã€TIPSï¼š\r\nä»“åº“ä¸­çš„ `docs/Interfaces.md` å·²è¿›è¡Œäº†ç®€è¦æ›´æ–°ï¼Œä¹Ÿå¯ä»¥æä¾›å‚è€ƒ",
        "state": "open",
        "user": "feifei-111",
        "closed_by": null,
        "created_at": "2023-06-20T03:06:56+00:00",
        "updated_at": "2023-06-20T11:20:36+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/PaDiff",
        "number": 83,
        "title": "æ›´æ–°setup.py",
        "body": "### ä¸€ã€é—®é¢˜æè¿° ğŸ“š\r\nPaDiff åœ¨æ›´æ–°æ–‡ä»¶ç»“æ„åï¼Œä½¿ç”¨ setup.py æ— æ³•æ­£ç¡®ä»æºç å®‰è£…ï¼ŒåŸå› æ˜¯datas æ–‡ä»¶å¤¹ä¸‹çš„æ•°æ®æ–‡ä»¶æ²¡æœ‰è¢«æ‰“åŒ…ã€‚\r\n\r\n### äºŒã€ä»»åŠ¡ç›®æ ‡ ğŸš€\r\næ›´æ–° PaDiff ä»“åº“çš„ setup.py æ–‡ä»¶ï¼Œæ­£ç¡®æ‰“åŒ… PaDiff å·¥å…·",
        "state": "closed",
        "user": "feifei-111",
        "closed_by": "feifei-111",
        "created_at": "2023-06-20T09:02:40+00:00",
        "updated_at": "2023-08-23T07:53:08+00:00",
        "closed_at": "2023-08-23T07:53:08+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/PaDiff",
        "number": 82,
        "title": "è¿‡æ»¤æ¡†æ¶ç»„ä»¶å†…éƒ¨çš„ API æ£€æŸ¥",
        "body": "### ä¸€ã€ é—®é¢˜æè¿°ğŸ“š\r\nPaDiff èƒ½å¤Ÿè¿›è¡Œ API çº§åˆ«çš„å¯¹é½æ£€æŸ¥ï¼Œå…¶åŸç†æ˜¯å°†æ¡†æ¶å†…çš„ç»„ç½‘ API åŒ…è£…æˆä¸€ä¸ªæ¨¡å‹ï¼Œç„¶åç»™è¿™ä¸ªæ¨¡å‹è®¾ç½® hookï¼Œè¿™ä½¿å¾—æ‰€æœ‰ API è°ƒç”¨å¿…å®šä¼šè¿›å…¥ hook é€»è¾‘ï¼Œå¯¹äºæ¡†æ¶æä¾›çš„æ¨¡å‹ç»„ä»¶è€Œè¨€ï¼ˆéç”¨æˆ·è‡ªå®šä¹‰ï¼‰ä¹Ÿæ˜¯å¦‚æ­¤ã€‚\r\n\r\n### äºŒã€ ä»»åŠ¡ç›®æ ‡ğŸš€\r\nä¿®æ”¹ PaDiff çš„ hook é€»è¾‘ï¼Œæ£€æŸ¥å½“å‰ API çš„è§¦å‘ä½ç½®ï¼Œè‹¥è¯¥ API è§¦å‘äº paddle/torch æ¡†æ¶æä¾›çš„æ¨¡å‹ç»„ä»¶ä¸‹ï¼Œåˆ™ç•¥è¿‡æ­¤ API çš„ä¿¡æ¯è®°å½•ã€‚\r\n\r\n### ä¸‰ã€TIPS\r\nç†Ÿæ‚‰ report æ¨¡å—ä¸‹çš„å†…å®¹ï¼Œç»“åˆ PaDiff å·¥å…·ç»´æŠ¤çš„æ ˆç»“æ„è¿›è¡Œåˆ¤æ–­",
        "state": "open",
        "user": "feifei-111",
        "closed_by": null,
        "created_at": "2023-06-20T08:55:22+00:00",
        "updated_at": "2023-06-20T11:22:05+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/PaDiff",
        "number": 79,
        "title": "åˆ é™¤yamlæ–‡ä»¶çš„å½±å“",
        "body": "### ä¸€ã€é—®é¢˜æè¿° ğŸ“š\r\nPaDiff ä»“åº“ä¸­æœ‰ä¸€ä»½ yaml æ–‡ä»¶ï¼ˆdatasæ–‡ä»¶å¤¹ä¸‹ï¼‰ï¼Œè¯¥æ–‡ä»¶æ ‡æ³¨äº†paddleä¸torchæä¾›çš„ç»„ä»¶æƒé‡ä¹‹é—´çš„å·®å¼‚ï¼Œä¾‹å¦‚linearçš„weightéœ€è¦è½¬ç½®æ‰èƒ½å¯¹é½ã€‚æ­¤å¤–ï¼Œåœ¨è¿›è¡Œå¯¹é½æ£€æŸ¥æ—¶è¿˜æœ‰ä¸€ä¸ªactionsæœºåˆ¶ï¼ˆåœ¨checkeræ–‡ä»¶å¤¹ä¸‹ï¼‰ï¼Œæ ¹æ®å½“å‰ä¼ å…¥çš„ç±»å‹åæ¥é€‰å–ä¸åŒçš„æ¯”è¾ƒå‡½æ•°ã€‚ç›®å‰è¿™ä¸¤ä¸ªæœºåˆ¶æ˜¯ç‹¬ç«‹çš„ï¼Œactionså®é™…ä¸Šåªæœ‰ä¸€ç§æ²¡æœ‰èµ·ä½œç”¨ã€‚\r\n\r\n### äºŒã€ ä»»åŠ¡ç›®æ ‡ ğŸš€\r\nä¿®æ”¹checkeræ¨¡å—ä¸‹ï¼Œå…³äºæ¨¡å‹æƒé‡ä»¥åŠæ¢¯åº¦çš„å¯¹é½é€»è¾‘ï¼Œå‰”é™¤yamlsçš„å½±å“ï¼Œæ”¹ä¸ºä½¿ç”¨actionsæœºåˆ¶ï¼ˆåŒæ—¶å¯ä»¥ä¼˜åŒ–ä¸€ä¸‹`get_action()`æ¥å£ï¼‰\r\n\r\nP.S. è¯¥ yaml æ–‡ä»¶åœ¨æƒé‡åˆå§‹åŒ–åŠŸèƒ½ä¸­ä»è¢«ä½¿ç”¨ï¼Œæƒé‡åˆå§‹åŒ–æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å—ã€‚å°±å¯¹é½å·¥å…·è€Œè¨€åªå¤„ç† checker æ¨¡å—ä¸‹çš„éƒ¨åˆ†å³å¯ã€‚\r\n\r\n### ä¸‰ã€ TIPS\r\nåœ¨è·å–actionsæ—¶ï¼Œéœ€è¦åŒºåˆ†å½“å‰çš„å¯¹é½ç›®æ ‡ï¼šé’ˆå¯¹æ¨¡å‹è¾“å‡º or æ¨¡å‹æƒé‡ã€‚ä¸åŒçš„å¯¹é½ç›®æ ‡åº”å½“å½±å“è¿”å›çš„ action ç±»å‹ï¼Œä¸ºæ­¤ï¼Œå¯èƒ½éœ€è¦ä¸º dump ä¸‹æ¥çš„æ–‡ä»¶æ·»åŠ é¢å¤–ä¿¡æ¯",
        "state": "open",
        "user": "feifei-111",
        "closed_by": null,
        "created_at": "2023-06-20T03:03:30+00:00",
        "updated_at": "2023-06-20T11:20:08+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/PaDiff",
        "number": 78,
        "title": "æ ‡è®°tensoråŠŸèƒ½",
        "body": "### ä¸€ã€é—®é¢˜æè¿° ğŸ“š\r\nåœ¨æ¨¡å‹ç²¾åº¦æ£€æŸ¥æ—¶ï¼Œæœ‰æ—¶åªéœ€è¦ç›‘æ§ç‰¹å®šçš„å‡ ä¸ªå…³é”® `Tensor` ã€‚æä¾›ä¸€ä¸ªæ¥å£ï¼Œç”¨æˆ·å°†è°ƒç”¨è¿™ä¸ªæ¥å£ä¾µå…¥å¼åœ°ä¿®æ”¹ç»„ç½‘ä»£ç ï¼Œæ ‡è®°éœ€è¦ç›‘æ§çš„ `Tensor` ã€‚è¿™ä¸ªæ¥å£ä¸å…¶ä»–æ¥å£æ˜¯äº’ç›¸ç‹¬ç«‹çš„ã€‚\r\n\r\n### äºŒã€ä»»åŠ¡ç›®æ ‡ ğŸš€\r\nè¯¥æ¥å£æ”¶åˆ°ä¸€ä¸ª `Tensor` åï¼Œåº”å½“ä¿å­˜è¿™ä¸ª `Tensor` çš„ä¿¡æ¯ï¼ˆåˆ°æŸä¸ªå…¨å±€å˜é‡/é—­åŒ…/æˆ–è€…å…¶ä»–æœºåˆ¶ï¼‰ï¼Œå¹¶æ³¨å†Œåå‘ `hook` ã€‚åœ¨ä¸€ä¸ª`step` åæä¾›æŸç§æœºåˆ¶ï¼Œå°†è¿™ä¸ª `step` ä¸­è®°å½•çš„ `Tensor` ä¿¡æ¯ä¿å­˜ä¸‹æ¥ã€‚\r\n\r\n### ä¸‰ã€ TIPS\r\nå…·ä½“çš„åšæ³•å¯ä»¥å‚è€ƒä»“åº“ä¸­ `report` æ–‡ä»¶å¤¹ä¸‹çš„ `report` ä¸ `tensor_hoo`k ä¹‹é—´çš„äº’åŠ¨ã€‚æœ€åï¼Œè¿˜éœ€è¦æä¾›ä¸€ä¸ª `dump` è®°å½•ä¿¡æ¯çš„æ¥å£ã€‚",
        "state": "open",
        "user": "feifei-111",
        "closed_by": null,
        "created_at": "2023-06-20T03:00:41+00:00",
        "updated_at": "2023-06-20T09:08:01+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/PaDiff",
        "number": 81,
        "title": "ğŸš€ PaDiff ç²¾åº¦å¯¹é½å·¥å…·å¿«ä¹å¼€æºä»»åŠ¡ ğŸ‰",
        "body": "### ä¸€ã€Background ğŸ“š\r\n\r\nPaDiffï¼Œå…¨ç§° Paddle Automatically Diff precision toolkits â€”â€” æ˜¯åŸºäº PaddlePaddle ä¸ PyTorch çš„æ¨¡å‹ç²¾åº¦å¯¹é½å·¥å…·ã€‚ä¼ å…¥ Paddle æˆ– Torch æ¨¡å‹ï¼Œå¯¹é½è®­ç»ƒä¸­é—´ç»“æœä»¥åŠè®­ç»ƒåçš„æ¨¡å‹æƒé‡ï¼Œå¹¶æç¤ºç²¾åº¦ diff ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®ã€‚\r\n\r\n+ æ–‡æ¡£ç›®å½• [Guides](https://github.com/PaddlePaddle/PaDiff/blob/develop/docs/README.md)\r\n+ ä½¿ç”¨æ•™ç¨‹ [Tutorial](https://github.com/PaddlePaddle/PaDiff/blob/develop/docs/Tutorial.md)\r\n+ å¯¹é½ViTPoseæµç¨‹ [ViTPose](https://github.com/PaddlePaddle/PaDiff/blob/develop/docs/CheckViTPose.md)\r\n+ æ¥å£å‚æ•°è¯´æ˜ [Interface](https://github.com/PaddlePaddle/PaDiff/blob/develop/docs/Interfaces.md)\r\n+ å¸¸è§é—®é¢˜è§£ç­” [FAQs](https://github.com/PaddlePaddle/PaDiff/blob/develop/docs/FAQs.md)\r\n\r\n\r\n### äºŒã€Motivation ğŸš€\r\n**å‚ä¸æœ¬é¡¹ç›®ï¼Œä½ å¯ä»¥ï¼š**\r\n- æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ æ¡†æ¶ ï¼ˆpaddle/torchï¼‰çš„è¿è¡Œæœºåˆ¶\r\n- æ·±å…¥äº†è§£æ¨¡å‹ç²¾åº¦å¯¹é½çš„åœºæ™¯éœ€æ±‚å’Œå¸¸è§é—®é¢˜\r\n\r\n\r\n### ä¸‰ã€Issue Tasks ğŸ™‹ğŸ»â€â™€ï¸\r\n\r\n| åºå· | ä»»åŠ¡å             | éš¾åº¦ |  issue                                               |  è®¤é¢†äºº | æäº¤PR |\r\n| :----: |:--------: |:----:|:----------------------:|:-----------:|:------:|\r\n| 1    |  æ–°å¢æ”¯æŒæ ‡è®° Tensor åŠŸèƒ½     | ä¸­ç­‰ | [#78](https://github.com/PaddlePaddle/PaDiff/issues/78) |       |        |\r\n| 2    | è§£è€¦ yaml é…ç½®æ–‡ä»¶ | ä¸­ç­‰ | [#79](https://github.com/PaddlePaddle/PaDiff/issues/79) |       |        |\r\n| 3    | è¿‡æ»¤æ¡†æ¶ç»„ä»¶å†…éƒ¨çš„ API æ£€æŸ¥ | ä¸­ç­‰ | [#82](https://github.com/PaddlePaddle/PaDiff/issues/82) |      |        |\r\n| 4    | æ›´æ–°å’Œå‡çº§ setup.py æ‰“åŒ…é€»è¾‘| ç®€å• | [#83](https://github.com/PaddlePaddle/PaDiff/issues/83) | @littsk | #84 \r\n\r\n> **Note**\r\n> å†³å®šè®¤é¢†ä»»åŠ¡åï¼Œè®°å¾—åŠæ—¶è”ç³»ç®¡ç†å‘˜å“¦~",
        "state": "open",
        "user": "feifei-111",
        "closed_by": null,
        "created_at": "2023-06-20T03:25:56+00:00",
        "updated_at": "2023-06-25T08:20:01+00:00",
        "closed_at": null,
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/PaDiff",
        "number": 98,
        "title": "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn",
        "body": "```python\r\nimport torch\r\n\r\nimport paddle\r\n\r\nimport numpy as np\r\n\r\nfrom padiff import create_model, auto_diff\r\n\r\n\r\nclass SparseDownSampleCloseBase(torch.nn.Module):\r\n    def __init__(self, stride):\r\n        super(SparseDownSampleCloseBase, self).__init__()\r\n        self.pooling = torch.nn.MaxPool2d(stride, stride)\r\n        self.large_number = 600\r\n\r\n    def forward(self, d, mask):\r\n        encode_d = -(1 - mask) * self.large_number - d\r\n\r\n        d = -self.pooling(encode_d)\r\n        mask_result = self.pooling(mask)\r\n        d_result = d - (1 - mask_result) * self.large_number\r\n\r\n        return d_result, mask_result\r\n\r\n\r\nclass SparseDownSampleCloseRaw(paddle.nn.Layer):\r\n    def __init__(self, stride):\r\n        super(SparseDownSampleCloseRaw, self).__init__()\r\n        self.pooling = paddle.nn.MaxPool2D(stride, stride)\r\n        self.large_number = 600\r\n\r\n    def forward(self, d, mask):\r\n        encode_d = -(1 - mask) * self.large_number - d\r\n\r\n        d = -self.pooling(encode_d)\r\n        mask_result = self.pooling(mask)\r\n        d_result = d - (1 - mask_result) * self.large_number\r\n\r\n        return d_result, mask_result\r\n\r\nmodule = create_model(SparseDownSampleCloseBase(1))\r\nlayer = create_model(SparseDownSampleCloseRaw(1))\r\n\r\nx = np.random.randn(1, 320, 320, 1).astype(\"float32\")\r\ny = np.random.randn(1, 320, 320, 1).astype(\"float32\")\r\ninp = ({\"d\": torch.as_tensor(x),\r\n        \"mask\": torch.as_tensor(y)},\r\n        {\"d\": paddle.to_tensor(x),\r\n        \"mask\": paddle.to_tensor(y)})\r\nauto_diff(module, layer, inp, auto_weights=True, atol=1e-4)\r\n\r\n```\r\n\r\n```\r\nVariable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n```",
        "state": "closed",
        "user": "GreatV",
        "closed_by": "GreatV",
        "created_at": "2023-08-23T07:01:12+00:00",
        "updated_at": "2023-09-04T02:41:30+00:00",
        "closed_at": "2023-09-04T02:41:30+00:00",
        "comments_count": [
            "feifei-111",
            "feifei-111",
            "GreatV",
            "feifei-111",
            "GreatV",
            "feifei-111",
            "feifei-111",
            "feifei-111",
            "GreatV"
        ],
        "labels": [
            "PFCC"
        ]
    },
    {
        "repo": "PaddlePaddle/PaDiff",
        "number": 97,
        "title": "MultiheadAttentionåˆå§‹åŒ–å¤±è´¥",
        "body": "åœ¨æ ·ä¾‹ä»£ç ä¸ŠåŠ å…¥MultiheadAttentionï¼Œå°è¯•è¿›è¡Œå‚æ•°å€¼å¤åˆ¶ï¼Œä½†å¤±è´¥\r\n\r\nç‰ˆæœ¬ï¼š\r\npaddlepaddle-gpu == 2.4.2\r\ntorch == 1.12.0+cu102\r\n\r\nä»£ç \r\n```\r\nimport paddle\r\nimport torch\r\nfrom padiff import assign_weight, create_model, auto_diff\r\nimport torch\r\nimport paddle\r\nimport paddle\r\nimport signal\r\nimport os\r\nfrom padiff import add_special_init\r\n\r\nclass SimpleModule(torch.nn.Module):\r\n  def __init__(self):\r\n      super(SimpleModule, self).__init__()\r\n      self.linear1 = torch.nn.Linear(100, 10)\r\n      self.attention = torch.nn.MultiheadAttention(64, 8, dropout=0.1, batch_first=True)\r\n  def forward(self, x):\r\n      x = self.linear1(x)\r\n      return x\r\n\r\nclass SimpleLayer(paddle.nn.Layer):\r\n  def __init__(self):\r\n      super(SimpleLayer, self).__init__()\r\n      self.linear1 = paddle.nn.Linear(100, 10)\r\n      self.attention = paddle.nn.MultiHeadAttention(64, 8, dropout=0.1)\r\n  def forward(self, x):\r\n      x = self.linear1(x)\r\n      return x\r\n  \r\nmodule = create_model(SimpleModule())\r\nmodule.auto_layer_map(\"base\")\r\nlayer = create_model(SimpleLayer())\r\nlayer.auto_layer_map(\"raw\")\r\n\r\nassign_weight(module, layer)\r\n```\r\n\r\næŠ¥é”™ï¼š\r\nRuntimeError:  Error occured when trying init weights, between:\r\n    base_model: `MultiheadAttention()`\r\n                `SimpleModule.attention.in_proj_weight`\r\n    raw_model: `Linear(in_features=64, out_features=64, dtype=float32)`\r\n               `SimpleLayer.attention.q_proj.weight`\r\n\r\næ¨¡å‹æ¶æ„æ—¥å¿—æ–‡ä»¶:\r\npadiff_log/weight_init_SimpleModule.log:\r\n```\r\nSimpleModule\r\n========================================\r\n    SimpleModule\r\n     |--- Linear\r\n     +--- MultiheadAttention    <---  *** HERE ***\r\n           +--- NonDynamicallyQuantizableLinear  (skip)\r\n\r\n```\r\npadiff_log/weight_init_SimpleLayer.log:\r\n\r\n```\r\nSimpleLayer\r\n========================================\r\n    SimpleLayer\r\n     |--- Linear\r\n     +--- MultiHeadAttention  (skip)\r\n           |--- Linear    <---  *** HERE ***\r\n           |--- Linear\r\n           |--- Linear\r\n           +--- Linear\r\n```\r\n\r\nè¯·é—®åº”è¯¥å¦‚ä½•ä¿®æ”¹å‘¢ï¼Ÿ",
        "state": "closed",
        "user": "ToddBear",
        "closed_by": "feifei-111",
        "created_at": "2023-08-14T09:17:01+00:00",
        "updated_at": "2023-08-23T07:27:20+00:00",
        "closed_at": "2023-08-23T07:27:20+00:00",
        "comments_count": [
            "feifei-111"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/PaDiff",
        "number": 95,
        "title": "TypeError: auto_diff() got an unexpected keyword argument 'atol'",
        "body": "ç‰ˆæœ¬ï¼š\r\n- torch == 1.11.0 + cu102\r\n- paddlepaddle-gpu == 2.4.2\r\n- padiff == 0.2.0\r\n\r\nè·‘ä¸‹é¢çš„demoä»£ç å‡ºé”™ï¼š\r\n\r\n```\r\nimport paddle\r\nimport torch\r\nfrom padiff import auto_diff\r\nimport torch\r\nimport paddle\r\n\r\nclass SimpleModule(torch.nn.Module):\r\n  def __init__(self):\r\n      super(SimpleModule, self).__init__()\r\n      self.linear1 = torch.nn.Linear(100, 10)\r\n  def forward(self, x):\r\n      x = self.linear1(x)\r\n      return x\r\n\r\nclass SimpleLayer(paddle.nn.Layer):\r\n  def __init__(self):\r\n      super(SimpleLayer, self).__init__()\r\n      self.linear1 = paddle.nn.Linear(100, 10)\r\n  def forward(self, x):\r\n      x = self.linear1(x)\r\n      return x\r\n\r\nmodule = SimpleModule()\r\nlayer = SimpleLayer()\r\n\r\ninp = paddle.rand((100, 100)).numpy().astype(\"float32\")\r\ninp = ({'x': torch.as_tensor(inp) },\r\n     {'x': paddle.to_tensor(inp)})\r\n\r\nauto_diff(module, layer, inp, atol=1e-4, auto_init=True)\r\n```\r\n\r\næŠ¥é”™ä¿¡æ¯å¦‚ä¸‹ï¼š\r\n```\r\nTraceback (most recent call last):\r\n  File \"padiff_test.py\", line 63, in <module>\r\n    auto_diff(module, layer, inp, atol=1e-4, auto_init=True)\r\nTypeError: auto_diff() got an unexpected keyword argument 'atol'\r\n```",
        "state": "closed",
        "user": "ToddBear",
        "closed_by": "ToddBear",
        "created_at": "2023-08-10T09:34:10+00:00",
        "updated_at": "2023-08-14T02:46:21+00:00",
        "closed_at": "2023-08-14T02:46:21+00:00",
        "comments_count": [
            "ToddBear"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/PaDiff",
        "number": 106,
        "title": "[AutoDiff] ==========Err occurs when compare grads!!!==========",
        "body": "## Versionç‰ˆæœ¬\r\n```shell\r\n> pip show padiff\r\nName: padiff\r\nVersion: 0.3.0\r\nSummary: A tools to automatically diff precision between Paddle and Pytorch Model.\r\n```\r\n## Run è¿è¡Œä»£ç ï¼š\r\n```\r\nbase_model_path_pd = \"Qwen/Qwen2-0.5B\"\r\nbase_model_path_hf = \"/media/wuyuhuan/Qwen2-0.5B_hf\"\r\n\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '1,3'\r\nos.environ['NVIDIA_TF32_OVERRIDE'] = '0'\r\nos.environ['FLAGS_embedding_deterministic'] = '1'\r\nos.environ['FLAGS_cudnn_deterministic'] = '1'\r\n\r\n\r\n\r\nimport numpy as np\r\nimport paddle\r\nimport torch\r\n\r\nfrom padiff import auto_diff\r\n\r\nfrom transformers import AutoConfig as AutoConfig_hf\r\nfrom transformers import AutoModelForCausalLM as AutoModelForCausalLM_hf\r\nfrom peft import LoKrConfig as LoKrConfig_hf, get_peft_model\r\nfrom peft.tuners.lokr import Linear as LoKrLinear_hf \r\n\r\n\r\nfrom paddlenlp.transformers import AutoConfig as AutoConfig_pd, AutoModelForCausalLM as AutoModelForCausalLM_pd\r\nfrom paddlenlp.peft.lokr import LoKrLinear as LoKrLinear_pd, LoKrConfig as LoKrConfig_pd, LoKrModel as LoKrModel_pd\r\n\r\nDEFAULT_TEST_CONFIG = {\r\n    \"base_model_name_or_path\": \"/home/wuyuhuan/.paddlenlp/models/Qwen/Qwen2-0.5B\",\r\n    \"target_modules\": [\".*q_proj*.\", \".*v_proj*.\"],\r\n    \"lokr_alpha\": 8,\r\n    \"lora_dim\": 8,\r\n    \"decompose_both\": False,\r\n    \"factor\": -1,\r\n}\r\n\r\ndef eval_model_convert():\r\n    print(\"CUDA_VISIBLE_DEVICES: \", os.environ['CUDA_VISIBLE_DEVICES'])\r\n    print(\"NVIDIA_TF32_OVERRIDE:\", os.environ['NVIDIA_TF32_OVERRIDE'])\r\n    print(\"FLAGS_embedding_deterministic:\", os.environ['FLAGS_embedding_deterministic'])\r\n    print(\"FLAGS_cudnn_deterministic:\", os.environ['FLAGS_cudnn_deterministic'])\r\n\r\n    paddle_input_ids = paddle.to_tensor([[0, 345, 232, 328, 740, 140, 1695, 69, 6078, 1588, 2]])\r\n    torch_input_ids = torch.LongTensor([[0, 345, 232, 328, 740, 140, 1695, 69, 6078, 1588, 2]])\r\n\r\n    # paddle model\r\n    paddle_ckpt_path = base_model_path_pd\r\n    config_paddle = AutoConfig_pd.from_pretrained(paddle_ckpt_path)\r\n    model_paddle = AutoModelForCausalLM_pd.from_pretrained(paddle_ckpt_path, config=config_paddle, dtype=\"float32\")\r\n\r\n    lokr_config_pd = LoKrConfig_pd(**DEFAULT_TEST_CONFIG)\r\n    model_paddle = LoKrModel_pd(model_paddle, lokr_config_pd)\r\n    # model_paddle = create_model(model_paddle, name=\"model_paddle\")\r\n                             \r\n    # torch model\r\n    torch_ckpt_path = base_model_path_hf\r\n    config_torch = AutoConfig_hf.from_pretrained(torch_ckpt_path, trust_remote_code=True)\r\n    config_torch.dtype = \"float32\"\r\n    model_torch = AutoModelForCausalLM_hf.from_pretrained(torch_ckpt_path, config=config_torch, trust_remote_code=True)\r\n\r\n    lokr_config_hf = LoKrConfig_hf(\r\n                target_modules=[\"q_proj\", \"v_proj\"],\r\n                r=8,\r\n                alpha=8,\r\n                decompose_factor = -1,\r\n                decompose_both = False)\r\n    \r\n    model_torch = get_peft_model(model_torch,lokr_config_hf,adapter_name=\"default\")\r\n    # model_torch = create_model(model_torch, name=\"model_torch\")\r\n\r\n    model_paddle.eval()\r\n    model_torch.eval()\r\n\r\n    # æ‰‹åŠ¨éªŒè¯\r\n    out_paddle = model_paddle(paddle_input_ids)[0]\r\n    out_torch = model_torch(torch_input_ids, return_dict=False)[0]\r\n    assert np.allclose(out_paddle.numpy(), out_torch.detach().numpy(), rtol=1e-5, atol=1e-4)\r\n    print(\"æ‰‹åŠ¨éªŒè¯å¯¹é½\")\r\n\r\n    # ä½¿ç”¨padifféªŒè¯\r\n    inp = [{\"input_ids\": torch_input_ids,\r\n            \"use_cache\": False,\r\n            \"output_attentions\": False,\r\n            \"output_hidden_states\": False,\r\n            \"return_dict\": False},\r\n        {\"input_ids\": paddle_input_ids}]\r\n    # diff_phase å¯ä»¥è®¾ç½®ä¸ºforwardï¼Œbackwordå’Œboth\r\n    auto_diff(model_torch, model_paddle, inp, atol = 1e-4, rtol = 1e3, diff_phase = \"both\", compare_mode = \"strict\")\r\n\r\neval_model_convert()\r\n```\r\n\r\n## é”™è¯¯ä¿¡æ¯\r\n```\r\n[2024-11-11 14:58:23,070] [    INFO] - Mark only lokr and trainable_module as trainable.\r\næ‰‹åŠ¨éªŒè¯å¯¹é½\r\n[AutoDiff] Your options:\r\n{\r\n  atol: `0.0001`\r\n  rtol: `1000.0`\r\n  auto_init: `False`\r\n  compare_mode: `strict`\r\n  single_step: `False`\r\n  use_loss: `False`\r\n  use_opt: `False`\r\n}\r\n[AutoDiff] check cfg {'atol': 0.0001, 'rtol': 1000.0, 'compare_mode': 'strict'}\r\n[AutoDiff] Checking report in /home/wuyuhuan/performance_compare/padiff_dump/PeftModel_base_model/auto_diff and /home/wuyuhuan/performance_compare/padiff_dump/LoKrModel_raw_model/auto_diff\r\n[AutoDiff] Check grads cfg: {'atol': 0.0001, 'rtol': 1000.0, 'compare_mode': 'strict'}\r\n[AutoDiff] Checking grads in /home/wuyuhuan/performance_compare/padiff_dump/PeftModel_base_model/auto_diff and /home/wuyuhuan/performance_compare/padiff_dump/LoKrModel_raw_model/auto_diff\r\n[AutoDiff] ==========Err occurs when compare grads!!!==========\r\n\r\nstring indices must be integers\r\n[AutoDiff] FAILED !!!\r\n```\r\n\r\næˆ‘æœç´¢æ‰¾ä¸åˆ°è¿™ä¸€è¡Œé”™è¯¯ä¿¡æ¯æ˜¯åœ¨å“ªé‡Œæ‰“å°çš„ã€‚çœ‹æ—¥å¿—ï¼Œå¯èƒ½æ˜¯æŸä¸€ä¸ªæ“ä½œæ“ä½œå‡ºç°äº†é—®é¢˜ã€‚\r\n\r\næŸ¥çœ‹äº†reportæ–‡ä»¶ä¹Ÿæ²¡æœ‰å…·ä½“è¯´æ˜å“ªé‡Œçš„gradå‡ºç°äº†é—®é¢˜ï¼Œå¸Œæœ›èƒ½å¤Ÿå¾—åˆ°åé¦ˆï¼\r\n",
        "state": "closed",
        "user": "WhuanY",
        "closed_by": "WhuanY",
        "created_at": "2024-11-11T08:23:36+00:00",
        "updated_at": "2024-11-13T08:23:25+00:00",
        "closed_at": "2024-11-13T08:23:24+00:00",
        "comments_count": [],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/PaDiff",
        "number": 99,
        "title": "from paddle.utils import flatten, pack_sequence_as, map_structure",
        "body": "åœ¨paddle2.4.2ä¸­è¿™å¥è¯æ‰§è¡Œå¤±è´¥ï¼Œè¯·é—®æœ‰é€‚é…2.4.2çš„padiffå—",
        "state": "open",
        "user": "WenmuZhou",
        "closed_by": null,
        "created_at": "2023-08-25T08:10:32+00:00",
        "updated_at": "2024-01-05T08:18:58+00:00",
        "closed_at": null,
        "comments_count": [
            "feifei-111"
        ],
        "labels": []
    },
    {
        "repo": "PaddlePaddle/PaDiff",
        "number": 100,
        "title": "åŒ…å«InstanceNorm2Dçš„æ¨¡å‹ï¼Œç”¨padiffä¼šæŠ¥é”™",
        "body": "```Python\r\nimport paddle\r\nimport paddle.nn as nn\r\n\r\nimport torch\r\n\r\nimport numpy as np\r\n\r\nfrom padiff import auto_diff, create_model\r\n\r\nimport paddle.nn as nn\r\n\r\nclass SimpleModel(nn.Layer):\r\n    def __init__(self,\r\n                 c1,\r\n                 c2,\r\n                 k=1,\r\n                 s=1,\r\n                 p=None,\r\n                 g=1,\r\n                 dropout_p=0.0):\r\n        super().__init__()\r\n        c_ = 1280\r\n        if p is None:\r\n            p = k // 2\r\n        self.conv = nn.Conv2D(c1, c_, k, s, padding=p, groups=g)\r\n        self.norm = nn.InstanceNorm2D(c_)\r\n        self.pool = nn.AdaptiveAvgPool2D(1)\r\n        self.drop = nn.Dropout(p=dropout_p)\r\n        self.linear = nn.Linear(c_, c2)\r\n\r\n    def forward(self, x):\r\n        if isinstance(x, list):\r\n            x = paddle.concat(x, 1)\r\n        return self.linear(self.drop(self.pool(self.norm(self.conv(x))).flatten(1)))\r\n\r\n\r\nmodel = SimpleModel(3, 10)\r\n\r\npaddle.summary(model, (1, 3, 224, 224))\r\n\r\nstate_dict = model.state_dict()\r\nfor key in state_dict:\r\n    print(key, state_dict[key].shape)\r\n\r\n\r\nclass SimpleModelRef(torch.nn.Module):\r\n    def __init__(self,\r\n                 c1,\r\n                 c2,\r\n                 k=1,\r\n                 s=1,\r\n                 p=None,\r\n                 g=1,\r\n                 dropout_p=0.0):\r\n        super().__init__()\r\n        c_ = 1280\r\n        if p is None:\r\n            p = k // 2\r\n        self.conv = torch.nn.Conv2d(c1, c_, k, s, padding=p, groups=g)\r\n        self.norm = torch.nn.InstanceNorm2d(c_)\r\n        self.pool = torch.nn.AdaptiveAvgPool2d(1)\r\n        self.drop = torch.nn.Dropout(p=dropout_p, inplace=True)\r\n        self.linear = torch.nn.Linear(c_, c2)\r\n\r\n    def forward(self, x):\r\n        if isinstance(x, list):\r\n            x = torch.cat(x, 1)\r\n        return self.linear(self.drop(self.pool(self.norm(self.conv(x))).flatten(1)))\r\n\r\n\r\n\r\nmodel = SimpleModelRef(3, 10)\r\n\r\nprint(\"---\" * 20)\r\nstate_dict = model.state_dict()\r\n\r\nfor key in state_dict:\r\n    print(key, state_dict[key].shape)\r\n\r\nmodule = create_model(SimpleModelRef(3, 10))\r\nmodule.auto_layer_map(\"base\")\r\n\r\nlayer = create_model(SimpleModel(3, 10))\r\nlayer.auto_layer_map(\"raw\")\r\n\r\ninput = np.random.randn(4, 3, 320, 320).astype(\"float32\")\r\ninp = ({\"x\": torch.as_tensor(input)}, {\"x\": paddle.to_tensor(input)})\r\nauto_diff(module, layer, inp, auto_weights=True)\r\n\r\n\r\n```\r\n\r\n```\r\n[AutoDiff] Auto set layer_map start searching...\r\n\r\n[AutoDiff] Auto set layer_map start searching...\r\n\r\n[AutoDiff] Your options:\r\n{\r\n  auto_init: `True`\r\n  single_step: `False`\r\n  use_loss: `False`\r\n  use_opt: `False`\r\n  atol: `0`\r\n  rtol: `1e-07`\r\n  compare_mode: `mean`\r\n}\r\n[AutoDiff] Assign weight Failed !!!\r\n\r\nRuntimeError:  Error occured when trying init weights, between:\r\n    base_model: `Linear(in_features=1280, out_features=10, bias=True)`\r\n                `SimpleModelRef.linear.weight`\r\n    raw_model: `InstanceNorm2D(num_features=1280, epsilon=1e-05)`\r\n               `SimpleModel.norm.scale`\r\nAssertionError:  Shape of param `weight` in torch::Linear and param `scale` in paddle::InstanceNorm2D is not the same. [10, 1280] vs [1280]\r\n\r\nWeight init log saved to \r\n    /home/greatx/repos/DocTrPP/padiff_log/weight_init_SimpleModelRef.log\r\n    /home/greatx/repos/DocTrPP/padiff_log/weight_init_SimpleModel.log\r\n\r\nPlease view the reports and checkout the layer marked with `<---  *** HERE ***` !\r\nHint:\r\n    1. Check the definition order of params is same in submodels.\r\n    2. Check the corresponding submodel have the same style:\r\n       param <=> param, buffer <=> buffer, embedding <=> embedding ...\r\n       cases like param <=> buffer, param <=> embedding are not allowed.\r\n    3. If can not change model codes, try to use a `LayerMap`\r\n       which can solve most problems.\r\n    4. (skip) means this layer is skipped because it is under black_list, or it has no param.\r\n    0. Visit `https://github.com/PaddlePaddle/PaDiff` to find more infomation.\r\n```",
        "state": "closed",
        "user": "GreatV",
        "closed_by": "GreatV",
        "created_at": "2023-09-28T03:43:11+00:00",
        "updated_at": "2023-10-20T05:56:57+00:00",
        "closed_at": "2023-10-20T05:56:57+00:00",
        "comments_count": [
            "GreatV",
            "feifei-111",
            "feifei-111"
        ],
        "labels": [
            "PFCC"
        ]
    }
]